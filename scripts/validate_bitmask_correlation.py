#!/usr/bin/env python3
"""
–ì–ª—É–±–æ–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –±–∏—Ç–æ–≤—ã—Ö –º–∞—Å–æ–∫ request ‚Üí response.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π parse_raw_packet –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –í–°–ï–• logical packets
–∏–∑ PutData, –≤–∫–ª—é—á–∞—è —Ç–µ, —á—Ç–æ Rust –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–∫–∞ –Ω–µ –ø–∞—Ä—Å–∏—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏.

Run: uv run python scripts/validate_bitmask_correlation.py
"""

from __future__ import annotations

import polars as pl
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict
import json

try:
    from km003c_lib import parse_raw_packet
    KM003C_LIB_AVAILABLE = True
except ImportError:
    print("‚ùå km003c_lib not available")
    exit(1)

from km003c_analysis.core import split_usb_transactions


def extract_all_logical_packets_from_raw(payload_hex: str) -> List[Dict[str, Any]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï logical packets –∏–∑ PutData –ø–∞–∫–µ—Ç–∞,
    –ø–∞—Ä—Å—è –∏—Ö –≤—Ä—É—á–Ω—É—é –∏–∑ raw bytes –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.
    """
    try:
        payload_bytes = bytes.fromhex(payload_hex)
        raw_packet = parse_raw_packet(payload_bytes)

        # –ù–æ–≤—ã–π dict-based API: Data/ SimpleData/ Ctrl –≤–∞—Ä–∏–∞–Ω—Ç—ã
        # –î–ª—è PutData (—Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏) –æ–∂–∏–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç "Data"
        if not (isinstance(raw_packet, dict) and "Data" in raw_packet):
            return []

        data_pkt = raw_packet["Data"]
        lps = data_pkt.get("logical_packets", [])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å payload_hex –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        logical_packets = []
        for lp in lps:
            payload = lp.get("payload", b"")
            logical_packets.append(
                {
                    "attribute": lp.get("attribute"),
                    "next": lp.get("next"),
                    "chunk": lp.get("chunk"),
                    "size": lp.get("size"),
                    "payload_hex": payload.hex() if isinstance(payload, (bytes, bytearray)) else "",
                }
            )

        return logical_packets
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error extracting logical packets: {e}")
        return []


def validate_bitmask_correlation():
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è: –±–∏—Ç–æ–≤—ã–µ –º–∞—Å–∫–∏ –≤ request –í–°–ï–ì–î–ê —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∞—Ç—Ä–∏–±—É—Ç–∞–º –≤ response.
    """
    
    dataset_path = Path("data/processed/usb_master_dataset.parquet")
    if not dataset_path.exists():
        print(f"‚ùå Dataset not found: {dataset_path}")
        return
    
    print("=" * 80)
    print("–ì–õ–£–ë–û–ö–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø –ë–ò–¢–û–í–û–ô –ö–û–†–†–ï–õ–Ø–¶–ò–ò request ‚Üí response")
    print("=" * 80)
    print()
    
    df = pl.read_parquet(dataset_path)
    
    # Filter for bulk transfers
    bulk_df = df.filter(
        (pl.col("transfer_type") == "0x03") &
        (pl.col("endpoint_address").is_in(["0x01", "0x81"]))
    )
    
    transactions = split_usb_transactions(bulk_df)
    
    # Track correlations
    correlation_data = {
        "total_pairs": 0,
        "perfect_matches": 0,
        "mismatches": [],
        "by_mask": defaultdict(lambda: {
            "total": 0,
            "expected_attributes": set(),
            "observed_attributes": defaultdict(int),
            "mismatch_cases": []
        }),
    }
    
    # Map bit position to attribute
    BIT_TO_ATTRIBUTE = {
        0: 1,      # bit 0 ‚Üí attribute 1 (ADC)
        1: 2,      # bit 1 ‚Üí attribute 2 (AdcQueue)
        3: 8,      # bit 3 ‚Üí attribute 8 (Settings)
        4: 16,     # bit 4 ‚Üí attribute 16 (PdPacket)
        9: 512,    # bit 9 ‚Üí attribute 512 (Unknown512)
    }
    
    # Pending requests by transaction_id
    pending_requests = {}
    
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏...")
    
    for row in transactions.iter_rows(named=True):
        if row["payload_hex"] is None or len(row["payload_hex"]) < 8:
            continue
        
        try:
            payload_bytes = bytes.fromhex(row["payload_hex"])
            raw_packet = parse_raw_packet(payload_bytes)
            
            is_request = row["endpoint_address"] == "0x01"
            is_response = row["endpoint_address"] == "0x81"
            
            if is_request and isinstance(raw_packet, dict) and "Ctrl" in raw_packet:
                ctrl = raw_packet["Ctrl"]
                header = ctrl.get("header", {})
                # 0x0C - GetData
                if header.get("packet_type") == 0x0C:
                    mask = header.get("attribute")
                    if mask is not None:
                        # Calculate expected attributes from mask
                        expected_attrs = set()
                        for bit_pos, attr_id in BIT_TO_ATTRIBUTE.items():
                            if mask & (1 << bit_pos):
                                expected_attrs.add(attr_id)

                        pending_requests[header.get("id")] = {
                            "mask": mask,
                            "mask_hex": f"0x{mask:04X}",
                            "expected_attributes": expected_attrs,
                            "timestamp": row["timestamp"],
                        }
            
            elif is_response and isinstance(raw_packet, dict) and "Data" in raw_packet:
                # Extract ALL logical packets
                logical_packets = extract_all_logical_packets_from_raw(row["payload_hex"])

                data_hdr = raw_packet["Data"]["header"]
                resp_id = data_hdr.get("id")

                if resp_id in pending_requests:
                    req = pending_requests.pop(resp_id)
                    
                    # Get observed attributes from logical packets
                    observed_attrs = set(lp["attribute"] for lp in logical_packets)
                    
                    correlation_data["total_pairs"] += 1
                    
                    mask = req["mask"]
                    mask_hex = req["mask_hex"]
                    expected = req["expected_attributes"]
                    
                    # Update statistics
                    correlation_data["by_mask"][mask_hex]["total"] += 1
                    correlation_data["by_mask"][mask_hex]["expected_attributes"] = expected
                    
                    observed_key = str(sorted(observed_attrs))
                    correlation_data["by_mask"][mask_hex]["observed_attributes"][observed_key] += 1
                    
                    # Check if perfect match
                    if observed_attrs == expected:
                        correlation_data["perfect_matches"] += 1
                    else:
                        # Mismatch!
                        mismatch = {
                            "mask": mask_hex,
                            "expected": sorted(expected),
                            "observed": sorted(observed_attrs),
                            "logical_packets": logical_packets,
                        }
                        correlation_data["mismatches"].append(mismatch)
                        correlation_data["by_mask"][mask_hex]["mismatch_cases"].append(mismatch)
        
        except Exception as e:
            # Skip parse errors
            pass
    
    # Report results
    print()
    print("=" * 80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò")
    print("=" * 80)
    print()
    
    total = correlation_data["total_pairs"]
    perfect = correlation_data["perfect_matches"]
    mismatches = len(correlation_data["mismatches"])
    
    print(f"–í—Å–µ–≥–æ request-response –ø–∞—Ä: {total:,}")
    print(f"–ò–¥–µ–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {perfect:,} ({perfect/total*100:.2f}%)")
    print(f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {mismatches} ({mismatches/total*100:.2f}%)")
    print()
    
    if perfect == total:
        print("‚úÖ ‚úÖ ‚úÖ –ò–î–ï–ê–õ–¨–ù–ê–Ø –ö–û–†–†–ï–õ–Ø–¶–ò–Ø!")
        print("   –ë–∏—Ç–æ–≤—ã–µ –º–∞—Å–∫–∏ –Ω–∞ 100% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∞—Ç—Ä–∏–±—É—Ç–∞–º –≤ –æ—Ç–≤–µ—Ç–∞—Ö!")
    else:
        print("‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ù–ï–°–û–í–ü–ê–î–ï–ù–ò–Ø")
    
    print()
    print("=" * 80)
    print("üìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–ê–°–ö–ê–ú")
    print("=" * 80)
    print()
    
    for mask_hex in sorted(correlation_data["by_mask"].keys(), key=lambda x: int(x, 16)):
        mask_data = correlation_data["by_mask"][mask_hex]
        
        print(f"–ú–∞—Å–∫–∞ {mask_hex}:")
        print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {mask_data['total']}")
        
        expected = mask_data['expected_attributes']
        if expected:
            print(f"  –û–∂–∏–¥–∞–µ–º—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã: {sorted(expected)}")
        else:
            print(f"  –û–∂–∏–¥–∞–µ–º—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã: [] (–ø—É—Å—Ç–∞—è –º–∞—Å–∫–∞)")
        
        print(f"  –ù–∞–±–ª—é–¥–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:")
        for observed_str, count in sorted(mask_data['observed_attributes'].items(), 
                                         key=lambda x: x[1], reverse=True):
            pct = count / mask_data['total'] * 100
            observed_set = set(eval(observed_str)) if observed_str != '[]' else set()
            match_marker = "‚úì" if observed_set == expected else "‚úó"
            print(f"    {match_marker} {observed_str}: {count} —Ä–∞–∑ ({pct:.1f}%)")
        
        if mask_data['mismatch_cases']:
            print(f"  ‚ö†Ô∏è  –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(mask_data['mismatch_cases'])}")
            for i, mismatch in enumerate(mask_data['mismatch_cases'][:3], 1):  # Show first 3
                print(f"     {i}. –û–∂–∏–¥–∞–ª–æ—Å—å {mismatch['expected']}, –ø–æ–ª—É—á–µ–Ω–æ {mismatch['observed']}")
        
        print()
    
    # Detailed mismatch analysis
    if correlation_data["mismatches"]:
        print("=" * 80)
        print("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ù–ï–°–û–í–ü–ê–î–ï–ù–ò–ô")
        print("=" * 80)
        print()
        
        for i, mismatch in enumerate(correlation_data["mismatches"], 1):
            print(f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ #{i}:")
            print(f"  –ú–∞—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {mismatch['mask']}")
            print(f"  –û–∂–∏–¥–∞–µ–º—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã: {mismatch['expected']}")
            print(f"  –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã: {mismatch['observed']}")
            print(f"  Logical packets –≤ –æ—Ç–≤–µ—Ç–µ:")
            for lp in mismatch['logical_packets']:
                print(f"    ‚Ä¢ attribute={lp['attribute']}, next={lp['next']}, size={lp['size']}, payload={lp['payload_hex'][:40]}...")
            print()
    
    # Export detailed results
    output_path = Path("data/processed/bitmask_correlation_validation.json")
    print(f"üíæ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤: {output_path}")
    
    # Convert defaultdicts and sets for JSON
    export_data = {
        "summary": {
            "total_pairs": correlation_data["total_pairs"],
            "perfect_matches": correlation_data["perfect_matches"],
            "mismatches_count": len(correlation_data["mismatches"]),
            "correlation_percentage": perfect / total * 100 if total > 0 else 0,
        },
        "mismatches": correlation_data["mismatches"],
        "by_mask": {
            mask: {
                "total": data["total"],
                "expected_attributes": sorted(data["expected_attributes"]),
                "observed_attributes": dict(data["observed_attributes"]),
                "mismatch_count": len(data["mismatch_cases"]),
            }
            for mask, data in correlation_data["by_mask"].items()
        }
    }
    
    with open(output_path, 'w') as f:
        json.dump(export_data, f, indent=2)
    
    print("   ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ!")
    print()
    
    print("=" * 80)
    if perfect == total:
        print("‚úÖ –í–ê–õ–ò–î–ê–¶–ò–Ø –ü–†–û–ô–î–ï–ù–ê: 100% –ö–û–†–†–ï–õ–Ø–¶–ò–Ø")
    else:
        print(f"‚ö†Ô∏è  –í–ê–õ–ò–î–ê–¶–ò–Ø: {perfect/total*100:.2f}% –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è")
    print("=" * 80)
    print()


if __name__ == "__main__":
    validate_bitmask_correlation()
